[
  {
    "role": "user",
    "content": "æˆ‘æƒ³å­¦spark"
  },
  {
    "role": "assistant",
    "content": "å¤ªå¥½äº†ï¼å­¦ä¹  Apache Spark æ˜¯ä¸€ä¸ªéå¸¸æœ‰å‰æ™¯çš„é€‰æ‹©ï¼Œå°¤å…¶æ˜¯åœ¨å¤§æ•°æ®å¤„ç†å’Œåˆ†å¸ƒå¼è®¡ç®—é¢†åŸŸã€‚Spark æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€æºé›†ç¾¤è®¡ç®—æ¡†æ¶ï¼Œå¹¿æ³›ç”¨äºæ•°æ®å¤„ç†ã€æœºå™¨å­¦ä¹ ã€æµå¤„ç†ç­‰åœºæ™¯ã€‚\n\nä¸‹é¢æ˜¯ä¸€ä¸ª**ä»é›¶å¼€å§‹å­¦ä¹  Spark çš„è·¯çº¿å›¾**ï¼Œå¸®åŠ©ä½ ç³»ç»Ÿåœ°æŒæ¡å®ƒï¼š\n\n---\n\n## ä¸€ã€åŸºç¡€å‡†å¤‡\n\n### 1. ç†Ÿæ‚‰ Java æˆ– Scala\n- **Java**ï¼šSpark æœ€åˆæ˜¯ç”¨ Scala ç¼–å†™çš„ï¼Œä½†ä¹Ÿå¯ä»¥ä½¿ç”¨ Java æˆ– Pythonã€‚\n- **Scala**ï¼šæ¨èä¼˜å…ˆå­¦ä¹ ï¼Œå› ä¸º Spark çš„ API æ›´è‡ªç„¶åœ°ä¸ Scala é›†æˆã€‚\n- **Pythonï¼ˆPySparkï¼‰**ï¼šå¦‚æœä½ æ›´ç†Ÿæ‚‰ Pythonï¼Œä¹Ÿå¯ä»¥ä» PySpark å¼€å§‹ã€‚\n\n### 2. å®‰è£… Java å’Œ JDK\n- å®‰è£… [JDK](https://www.oracle.com/java/technologies/downloads/)ï¼ˆå»ºè®® Java 8 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰\n- è®¾ç½®ç¯å¢ƒå˜é‡ `JAVA_HOME`\n\n### 3. å®‰è£… Scalaï¼ˆå¯é€‰ï¼‰\n- å¦‚æœä½ æ‰“ç®—ç”¨ Scala ç¼–å†™ Spark ç¨‹åºï¼Œå¯ä»¥å®‰è£… [Scala](https://www.scala-lang.org/download/)\n\n### 4. å®‰è£… Spark\n- ä¸‹è½½ [Apache Spark](https://spark.apache.org/downloads.html)\n- è§£å‹å¹¶è®¾ç½®ç¯å¢ƒå˜é‡ `SPARK_HOME`\n- å¯ä»¥ä½¿ç”¨ `spark-shell` è¿›è¡Œäº¤äº’å¼å¼€å‘\n\n---\n\n## äºŒã€å­¦ä¹  Spark æ ¸å¿ƒæ¦‚å¿µ\n\n### 1. ä»€ä¹ˆæ˜¯ Sparkï¼Ÿ\n- åˆ†å¸ƒå¼è®¡ç®—å¼•æ“\n- æ”¯æŒå†…å­˜è®¡ç®—ï¼Œé€Ÿåº¦æ¯” Hadoop MapReduce å¿«å¾ˆå¤š\n- æä¾› RDDï¼ˆå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼‰ã€DataFrameã€Dataset ç­‰ API\n\n### 2. åŸºæœ¬æ¶æ„\n- Driver Program\n- Executor\n- Cluster Managerï¼ˆå¦‚ YARN, Mesos, Kubernetesï¼‰\n\n### 3. RDDï¼ˆResilient Distributed Datasetï¼‰\n- Spark æœ€æ ¸å¿ƒçš„æ•°æ®ç»“æ„\n- æ”¯æŒè½¬æ¢æ“ä½œï¼ˆTransformationsï¼‰å’ŒåŠ¨ä½œæ“ä½œï¼ˆActionsï¼‰\n\n---\n\n## ä¸‰ã€åŠ¨æ‰‹å®è·µ\n\n### 1. ä½¿ç”¨ spark-shell äº¤äº’å¼å­¦ä¹ \n```bash\n$ cd $SPARK_HOME\n$ ./bin/spark-shell\n```\n\n#### ç¤ºä¾‹ä»£ç ï¼š\n```scala\nval data = Array(1, 2, 3, 4, 5)\nval distData = sc.parallelize(data)\ndistData.map(_ * 2).collect()\n```\n\n### 2. ç¼–å†™ç¬¬ä¸€ä¸ª Spark åº”ç”¨ç¨‹åºï¼ˆScalaï¼‰\n```scala\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\n\nobject WordCount {\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setAppName(\"WordCount\")\n    val sc = new SparkContext(conf)\n\n    val lines = sc.textFile(\"file:///path/to/your/file.txt\")\n    val words = lines.flatMap(line => line.split(\" \"))\n    val wordCounts = words.map(word => (word, 1)).reduceByKey(_ + 1)\n\n    wordCounts.saveAsTextFile(\"output_path\")\n    sc.stop()\n  }\n}\n```\n\n### 3. ä½¿ç”¨ PySparkï¼ˆPython ç‰ˆæœ¬ï¼‰\n```python\nfrom pyspark import SparkContext\n\nsc = SparkContext(\"local\", \"WordCount\")\nlines = sc.textFile(\"file:///path/to/your/file.txt\")\nwords = lines.flatMap(lambda line: line.split())\nword_counts = words.map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)\nword_counts.saveAsTextFile(\"output_path\")\n```\n\n---\n\n## å››ã€è¿›é˜¶å†…å®¹\n\n### 1. DataFrame å’Œ Dataset\n- æ›´é«˜æ•ˆçš„ APIï¼Œé€‚åˆç»“æ„åŒ–æ•°æ®\n- æ”¯æŒ SQL æŸ¥è¯¢\n- æ¨èåœ¨å®é™…é¡¹ç›®ä¸­ä½¿ç”¨\n\n### 2. Spark SQL\n- å¤„ç†ç»“æ„åŒ–æ•°æ®\n- ä¸ Hive é›†æˆ\n- æ”¯æŒ SQL æŸ¥è¯¢å’Œ DataFrame API\n\n### 3. Spark Streaming\n- å®æ—¶æ•°æ®å¤„ç†\n- æ”¯æŒ Kafkaã€Flume ç­‰æ•°æ®æº\n\n### 4. Spark MLlib\n- æœºå™¨å­¦ä¹ åº“\n- åŒ…å«åˆ†ç±»ã€èšç±»ã€å›å½’ç­‰ç®—æ³•\n\n### 5. Spark GraphX\n- å›¾è®¡ç®—æ¡†æ¶\n- ç”¨äºç¤¾äº¤ç½‘ç»œã€æ¨èç³»ç»Ÿç­‰\n\n### 6. è°ƒä¼˜ä¸æ€§èƒ½ä¼˜åŒ–\n- å†…å­˜ç®¡ç†\n- æ•°æ®åˆ†åŒº\n- Shuffle ä¼˜åŒ–\n- ä½¿ç”¨ç¼“å­˜ï¼ˆcache / persistï¼‰\n\n---\n\n## äº”ã€æ¨èèµ„æº\n\n### ä¹¦ç±ï¼š\n- ã€ŠLearning Sparkã€‹ â€“ O'Reilly\n- ã€ŠSpark: The Definitive Guideã€‹ â€“ Manning Publications\n\n### åœ¨çº¿è¯¾ç¨‹ï¼š\n- [Coursera - Apache Spark](https://www.coursera.org/specializations/apache-spark)\n- [Udemy - Spark for Beginners](https://www.udemy.com/)\n- [edX - Big Data with Spark](https://www.edx.org/)\n\n### å®˜æ–¹æ–‡æ¡£ï¼š\n- [Spark å®˜æ–¹æ–‡æ¡£](https://spark.apache.org/docs/latest/)\n\n---\n\n## å…­ã€å¸¸è§é—®é¢˜\n\n### Q: Spark å’Œ Hadoop çš„åŒºåˆ«ï¼Ÿ\nA: Spark æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ï¼ŒHadoop æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼å­˜å‚¨å’Œè®¡ç®—å¹³å°ã€‚Spark å¯ä»¥è¿è¡Œåœ¨ Hadoop ä¸Šï¼Œä½†ä¹Ÿå¯ä»¥ç‹¬ç«‹è¿è¡Œã€‚\n\n### Q: Spark é€‚åˆä»€ä¹ˆåœºæ™¯ï¼Ÿ\nA: å¤§æ•°æ®æ‰¹å¤„ç†ã€å®æ—¶æµå¤„ç†ã€æœºå™¨å­¦ä¹ ã€å›¾è®¡ç®—ç­‰ã€‚\n\n---\n\nå¦‚æœä½ å‘Šè¯‰æˆ‘ä½ ç›®å‰çš„ç¼–ç¨‹èƒŒæ™¯ï¼ˆæ¯”å¦‚æ˜¯å¦æ‡‚ Java/Python/Scalaï¼‰ï¼Œæˆ‘å¯ä»¥ä¸ºä½ å®šåˆ¶ä¸€ä¸ªæ›´å…·ä½“çš„å­¦ä¹ è®¡åˆ’ ğŸ˜Š\n\néœ€è¦æˆ‘å¸®ä½ è§„åˆ’ä¸€ä¸‹å­¦ä¹ è·¯å¾„å—ï¼Ÿ"
  }
]